{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from src.utils import *\n",
    "from src.model import *\n",
    "from src.data_loader import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load hyperparameters from JSON file\n",
    "def load_hyperparameters(json_path=\"best_hyperparameters.json\"):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        hyperparams = json.load(f)\n",
    "    return hyperparams\n",
    "\n",
    "# Preprocess data and create data loaders\n",
    "def preprocess(config, hyperparams):\n",
    "    mean, std = compute_mean_std(config[\"train_dir\"])\n",
    "\n",
    "    train_transformer, test_transformer = create_transformers(\n",
    "        mean,\n",
    "        std,\n",
    "        config[\"height\"],\n",
    "        config[\"width\"],\n",
    "        config[\"random_rotation_degrees\"],\n",
    "        config[\"random_affine_degrees\"],\n",
    "        config[\"random_translation\"],\n",
    "        config[\"brightness\"],\n",
    "        config[\"contrast\"],\n",
    "        config[\"saturation\"],\n",
    "        config[\"hue\"]\n",
    "    )\n",
    "\n",
    "    train_data = ImageFolder(config[\"train_dir\"], transform=train_transformer)\n",
    "    test_data = ImageFolder(config[\"test_dir\"], transform=test_transformer)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=hyperparams[\"batch_size\"], shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=hyperparams[\"batch_size\"], shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Calculate top-k accuracy\n",
    "def topk_accuracy(output, target, k):\n",
    "    with torch.no_grad():\n",
    "        _, pred = output.topk(k, dim=1, largest=True, sorted=True)\n",
    "        correct = pred.eq(target.view(-1, 1).expand_as(pred))\n",
    "        return correct.any(dim=1).float().mean().item()\n",
    "\n",
    "# Training function\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch, config):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    \n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    total_loss = 0\n",
    "    topk_correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "\n",
    "        topk_correct += topk_accuracy(y_pred, target, k=config[\"topk\"]) * len(data)\n",
    "\n",
    "        pbar.set_description(desc=f'Loss={loss.item():.4f} Batch={batch_idx} Accuracy={100 * correct / processed:.2f}%')\n",
    "\n",
    "    epoch_train_loss = total_loss / len(train_loader)\n",
    "    epoch_train_acc = 100 * correct / processed\n",
    "    epoch_topk_acc = 100 * topk_correct / processed\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_acc.append(epoch_train_acc)\n",
    "    train_topk_acc.append(epoch_topk_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Loss={epoch_train_loss:.4f}, Accuracy={epoch_train_acc:.2f}%, Top-{config[\"topk\"]} Accuracy={epoch_topk_acc:.2f}%')\n",
    "\n",
    "# Evaluation function\n",
    "def test(model, device, test_loader, criterion, config):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    topk_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            topk_correct += topk_accuracy(output, target, k=config[\"topk\"]) * len(data)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
    "    test_topk_acc.append(100. * topk_correct / len(test_loader.dataset))\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%), Top-{} Accuracy: {:.2f}%\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset), config[\"topk\"],\n",
    "        100. * topk_correct / len(test_loader.dataset)\n",
    "    ))\n",
    "\n",
    "# Save training and evaluation plots\n",
    "def save_plots(train_losses, train_acc, test_losses, test_acc, train_topk_acc, test_topk_acc, topk):\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plot_path = os.path.join(\"plots\", f\"training_results_{timestamp}.png\")\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
    "    \n",
    "    axs[0].plot(train_losses, label='Train Loss', color='green')\n",
    "    axs[0].plot(test_losses, label='Test Loss', color='red')\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(train_acc, label='Train Accuracy', color='green')\n",
    "    axs[1].plot(test_acc, label='Test Accuracy', color='red')\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "    \n",
    "    axs[2].plot(train_topk_acc, label=f'Train Top-{topk} Accuracy', color='blue')\n",
    "    axs[2].plot(test_topk_acc, label=f'Test Top-{topk} Accuracy', color='purple')\n",
    "    axs[2].set_title(f\"Top-{topk} Accuracy\")\n",
    "    axs[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    print(f\"Plots saved to {plot_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hyperparams = load_hyperparameters(\"best_hyperparameters.json\")\n",
    "\n",
    "    config = load_config(\"config.yaml\")\n",
    "\n",
    "    train_loader, test_loader = preprocess(config, hyperparams)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_topk_acc = []\n",
    "    test_topk_acc = []\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = ResNet18().to(device)\n",
    "\n",
    "    if hyperparams[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=hyperparams[\"lr\"],\n",
    "            momentum=hyperparams[\"momentum\"],\n",
    "            weight_decay=hyperparams[\"weight_decay\"]\n",
    "        )\n",
    "    elif hyperparams[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=hyperparams[\"lr\"])\n",
    "    elif hyperparams[\"optimizer\"] == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=hyperparams[\"lr\"])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=hyperparams[\"step_size\"],\n",
    "        gamma=hyperparams[\"gamma\"]\n",
    "    )\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and evaluation loop\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch, config)\n",
    "        scheduler.step()\n",
    "        print('Current Learning Rate: ', optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "        test(model, device, test_loader, criterion, config)\n",
    "\n",
    "    # Save plots\n",
    "    save_plots(train_losses, train_acc, test_losses, test_acc, train_topk_acc, test_topk_acc, config[\"topk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
